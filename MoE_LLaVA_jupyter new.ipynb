{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gesen2egee/MoE-LLaVA-hf/blob/main/MoE_LLaVA_jupyter%20new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**moe模型DEMO**\n",
        "https://huggingface.co/spaces/LanguageBind/MoE-LLaVA\n",
        "\n",
        "\n",
        "**WD14模型DEMO**\n",
        "https://huggingface.co/spaces/SmilingWolf/wd-tagger\n"
      ],
      "metadata": {
        "id": "UKL6UUw8mCa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **先[ ]執行上面區塊 再執行下面區塊**"
      ],
      "metadata": {
        "id": "1d2PIDtjJvl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/gesen2egee/MoE-LLaVA-hf\n",
        "\n",
        "#在其他linux系統或runpod安裝mpi4py需要\n",
        "#!apt-get update\n",
        "#!apt-get install -y libopenmpi-dev openmpi-bin\n",
        "\n",
        "%cd /content/MoE-LLaVA-hf\n",
        "!pip install -e .\n",
        "%cd /content/MoE-LLaVA-hf\n",
        "!pip install deepspeed==0.12.6 gradio==3.50.2 decord==0.6.0 transformers==4.37.0 einops timm tiktoken accelerate mpi4py\n",
        "\n",
        "%cd /content/MoE-LLaVA-hf\n",
        "#!python app.py #測試moe用的GUI介面"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jBE5F1vOlJ2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "將py後面 ' '內YOUR_DATASET'S_COLAB_PATH換成你的資料集路徑(左邊可以用目錄右鍵複製路徑，掛載google雲端在預設/content/drive/MyDrive下)\n",
        "\n",
        "--enable_wildcard 打成多行\n",
        "\n",
        "\n",
        "(要在在kohya中使用多行wildcard要在引數加上 --enable_wildcard\n",
        "不然預設只用第一行當caption)\n",
        "\n",
        "--folder_name 將子資料夾_之後字串，放到打標之前當作觸發詞\n",
        "\n",
        "\n",
        "--not_char folder_name不是角色 會用概念的方式打"
      ],
      "metadata": {
        "id": "uLzTOqPpc9n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pBfVTmJjgRWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull\n",
        "%cd /content/MoE-LLaVA-hf\n",
        "!python predict2.py '/content/drive/MyDrive/train' --folder_name --enable_wildcard"
      ],
      "metadata": {
        "id": "5fxSbjAMAiyq",
        "outputId": "8b89e31b-9c8c-4ef0-e57f-03ea54686ecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 955 bytes | 955.00 KiB/s, done.\n",
            "From https://github.com/gesen2egee/MoE-LLaVA-hf\n",
            "   9f3ee34..b3e1f8c  main       -> origin/main\n",
            "Updating 9f3ee34..b3e1f8c\n",
            "Fast-forward\n",
            " predict2.py | 6 \u001b[32m+++\u001b[m\u001b[31m---\u001b[m\n",
            " 1 file changed, 3 insertions(+), 3 deletions(-)\n",
            "/content/MoE-LLaVA-hf\n",
            "2024-07-18 11:22:38.820331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-18 11:22:38.820384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-18 11:22:38.821836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-18 11:22:38.832361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 11:22:40.683694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-07-18 11:22:43,222] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-07-18 11:22:45,560] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,562] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,565] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,567] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,569] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,571] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,573] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,575] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,577] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,580] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,582] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,584] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,586] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,588] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,590] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "[2024-07-18 11:22:45,592] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
            "Loading checkpoint shards: 100% 3/3 [00:46<00:00, 15.51s/it]\n",
            "[2024-07-18 11:23:32,561] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-07-18 11:23:32,562] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2024-07-18 11:23:33,254] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n",
            "[2024-07-18 11:23:33,254] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2024-07-18 11:23:33,257] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
            "[2024-07-18 11:23:33,259] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "處理圖片 /content/drive/MyDrive/train: 0it [00:00, ?it/s]\n",
            "處理圖片 /content/drive/MyDrive/train/1_page:   0% 0/69 [00:00<?, ?it/s]\u001b[1;31m2024-07-18 11:23:50.821484246 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-18 11:23:50.821521612 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "處理圖片 /content/drive/MyDrive/train/1_page:  59% 41/69 [10:26<06:44, 14.46s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用方式\n",
        "\n",
        "1.   將資料及colab位置填入，預設folder_name會將重複次數_後面的字串當作角色名觸發詞，如果不是訓練角色，可以在指令再加上--not_char會用目錄名當作概念觸發詞\n",
        "\n",
        "2.   如果要在kohya中使用wildcard訓練，在Additional parameters中加上--enable_wildcard，會使用wildcard隨機選一行當作caption，不然預設只用第一行\n",
        "3.   如果想對WD14標籤洗牌或tag dropout，在Additional parameters加上--keep_tokens_separator=\"___\"，會只針對WD14標籤處理\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-jhpQX3FWvay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**本機安裝方式**\n",
        "\n",
        "建議使用WSL或LINUX安裝\n",
        "\n",
        "安裝方式可以參考\n",
        "\n",
        "https://github.com/PKU-YuanGroup/MoE-LLaVA\n",
        "\n",
        "https://github.com/camenduru/MoE-LLaVA-hf\n",
        "\n",
        "跑圖可以找我修改的腳本\n",
        "\n",
        "https://github.com/gesen2egee/MoE-LLaVA-hf/blob/main/predict2.py\n",
        "\n",
        "建議VRAM<16G改成 MOE_MODEL_PATH = 'LanguageBind/MoE-LLaVA-StableLM-1.6B-4e-384'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sCUeD2LBcFsI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlMxC_lucPgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}